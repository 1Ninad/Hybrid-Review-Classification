import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import torch
from torch_geometric.data import Data
from torch_geometric.nn import GAE, GCNConv

# Load datasets
labeled_data = pd.read_csv('3k_labeled_extracted.csv')
unlabeled_data = pd.read_csv('30k_unlabeled_extracted.csv')

# Prepare features and labels for labeled data
X_labeled = labeled_data.drop(columns=['text_', 'label']).values
y_labeled = labeled_data['label'].values

# For unlabeled data, use only features
X_unlabeled = unlabeled_data.drop(columns=['text_']).values

# Encode labels (e.g., 'CG' as 0, 'OR' as 1)
label_encoder = LabelEncoder()
y_labeled_encoded = label_encoder.fit_transform(y_labeled)

# Concatenate both datasets for graph creation
X_combined = np.vstack((X_labeled, X_unlabeled))

from sklearn.neighbors import kneighbors_graph

def create_graph(X, n_neighbors=5):
    # Create a knearest neighbor graph
    adj_matrix = kneighbors_graph(X, n_neighbors=n_neighbors, mode='connectivity', include_self=False)
    edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)
    x = torch.tensor(X, dtype=torch.float)
    return Data(x=x, edge_index=edge_index)

# Create graph data for labeled and unlabeled datasets
graph_data = create_graph(X_combined, n_neighbors=5)


class GCN(torch.nn.Module):
    def __init__(self, input_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, 16)
        self.conv2 = GCNConv(16, 8)

    def encode(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = torch.relu(x)
        x = self.conv2(x, edge_index)
        return x

    def decode(self, z, edge_index):
        return torch.sigmoid(torch.matmul(z[edge_index[0]], z[edge_index[1]].T))

# Instantiate model
model = GCN(input_dim=X_combined.shape[1])


import torch
from torch.optim import Adam
import time

# Set the device to CPU
device = torch.device('cpu')

def train_gae(model, data, epochs=20, lr=0.01, early_stopping_patience=5, log_frequency=2):
    # Move model to CPU
    model.to(device)
    
    optimizer = Adam(model.parameters(), lr=lr)
    best_loss = float('inf')
    patience_counter = 0

    model.train()
    start_time = time.time()  # Timing for monitoring

    for epoch in range(epochs):
        optimizer.zero_grad()
        
        # Forward pass
        z = model.encode(data.x.to(device), data.edge_index.to(device))  # Ensure data is on CPU
        loss = model.decode(z, data.edge_index.to(device)).mean()

        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Log loss at specified intervals
        if epoch % log_frequency == 0:
            print(f'Epoch {epoch}: Loss {loss.item():.4f}')

        # Early stopping check
        if loss < best_loss:
            best_loss = loss
            patience_counter = 0  # Reset counter if we improved
        else:
            patience_counter += 1
            if patience_counter >= early_stopping_patience:
                print("Early stopping triggered.")
                break

    end_time = time.time()
    print(f"Training completed in {end_time - start_time:.2f} seconds.")

# Train the model with the CPU
train_gae(model, graph_data)



# Step 1: Extract embeddings for unlabeled data using the same graph model
with torch.no_grad():
    num_labeled = len(y_labeled_encoded)
    all_embeddings = model.encode(
        graph_data.x.to(device), 
        graph_data.edge_index.to(device)  # Use the full edge_index
    ).cpu().numpy()
labeled_embeddings = all_embeddings[:num_labeled]
unlabeled_embeddings = all_embeddings[num_labeled:]
unlabeled_predictions = classifier.predict(unlabeled_embeddings)
unlabeled_data['predicted_label'] = label_encoder.inverse_transform(unlabeled_predictions)
unlabeled_data.to_csv('2_unlabeled_with_predictions.csv', index=False)

# Print out some of the predictions for verification
print(unlabeled_data[['text_', 'predicted_label']].head())




import torch
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_recall_curve
import xgboost as xgb
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import VotingClassifier

# Prepare your data
model.eval()
with torch.no_grad():
    embeddings = model.encode(graph_data.x.to(device), graph_data.edge_index.to(device)).cpu().numpy()

labels = y_labeled_encoded
X_train, X_test, y_train, y_test = train_test_split(embeddings[:len(y_labeled_encoded)], labels, test_size=0.3, random_state=42)

# Create and train classifiers
xgb_classifier = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, eval_metric='mlogloss')
mlp_classifier = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, alpha=0.001, random_state=42)

voting_classifier = VotingClassifier(estimators=[('xgb', xgb_classifier), ('mlp', mlp_classifier)], voting='soft')
voting_classifier.fit(X_train, y_train)

# Make predictions
y_pred = voting_classifier.predict(X_test)

# Evaluate the classifier
print("Classification Report:\n", classification_report(y_test, y_pred, target_names=label_encoder.classes_, digits=3))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Accuracy Score: {:.3f}".format(accuracy_score(y_test, y_pred)))
print("F1 Score: {:.3f}".format(f1_score(y_test, y_pred, average='weighted')))

# Enhanced Confusion Matrix Visualization
conf_matrix = confusion_matrix(y_test, y_pred)

# 4K Confusion Matrix
plt.figure(figsize=(38.4, 21.6))  # 4K size
sns.set(font_scale=2)  # Increase font size
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_,
            linewidths=0.5, linecolor='gray', cbar_kws={"shrink": .8})
plt.title('Confusion Matrix', fontsize=64)
plt.xlabel('Predicted Label', fontsize=48)
plt.ylabel('True Label', fontsize=48)

# Improve aesthetics
plt.xticks(rotation=45, ha='right', fontsize=40)
plt.yticks(rotation=0, fontsize=40)
plt.grid(False)  # Remove grid for better visibility

# Save the confusion matrix in 4K with 300 DPI
plt.savefig('confusion_matrix_4k.png', dpi=300, bbox_inches='tight')
plt.show()

# Precision-Recall Curve Visualization for 4K
y_scores = voting_classifier.predict_proba(X_test)[:, 1]  # Assuming binary classification; adjust as necessary
precision, recall, _ = precision_recall_curve(y_test, y_scores, pos_label=1)

# 4K Precision-Recall Curve
plt.figure(figsize=(38.4, 21.6))  # 4K size
plt.plot(recall, precision, marker='.', label='Voting Classifier', color='orange', linewidth=6)
plt.title('Precision-Recall Curve', fontsize=64)
plt.xlabel('Recall', fontsize=48)
plt.ylabel('Precision', fontsize=48)
plt.legend(fontsize=40)
plt.grid()
plt.xlim(0, 1)
plt.ylim(0, 1)

# Save the precision-recall curve in 4K with 300 DPI
plt.savefig('precision_recall_curve_4k.png', dpi=300, bbox_inches='tight')
plt.show()




import torch
import pandas as pd
from collections import Counter

# Code Cell 6 - Predicting Labels for Unlabeled Data

# Step 1: Set the model to evaluation mode
model.eval()  

# Step 2: Extract embeddings for unlabeled data using the same graph model
with torch.no_grad():
    # Get the total number of labeled samples
    num_labeled = len(y_labeled_encoded)
    
    # Extract all embeddings from the graph model
    all_embeddings = model.encode(
        graph_data.x.to(device), 
        graph_data.edge_index.to(device)  # Use the complete edge_index for the graph
    ).cpu().numpy()

# Step 3: Separate labeled and unlabeled embeddings
labeled_embeddings = all_embeddings[:num_labeled]  # First num_labeled embeddings
unlabeled_embeddings = all_embeddings[num_labeled:]  # Remaining embeddings for unlabeled data

# Step 4: Make predictions on unlabeled data using the trained classifier
unlabeled_predictions = classifier.predict(unlabeled_embeddings)

# Step 5: Add predicted labels to the unlabeled data DataFrame
unlabeled_data['predicted_label'] = label_encoder.inverse_transform(unlabeled_predictions)

# Step 6: Save the predictions to a CSV file
unlabeled_data.to_csv('3_unlabeled_data_with_predictions.csv', index=False)

# Step 7: Print a summary of predictions for verification
print("Predictions for Unlabeled Data:")
print(unlabeled_data[['text_', 'predicted_label']].head())

# Step 8: Count the occurrences of each predicted label
predicted_label_counts = Counter(unlabeled_data['predicted_label'])
print("\nTotal number of predictions made for each label:")
for label, count in predicted_label_counts.items():
    print(f"{label}: {count}")

# Optional: Print the total number of predictions made
print(f"\nTotal predictions made for unlabeled data: {len(unlabeled_data)}")
