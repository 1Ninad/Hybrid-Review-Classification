{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e47e02d-8a04-4412-bbb2-ca2cba2181a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph data saved to 'graphsage_processed_data.pt'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "# Load dataset\n",
    "labeled_data = pd.read_csv('bert_vader.csv')\n",
    "\n",
    "# Drop text columns or irrelevant fields\n",
    "labeled_data = labeled_data.drop(columns=['review'])\n",
    "\n",
    "X = labeled_data.drop(columns=['label']).values\n",
    "y = labeled_data['label'].values\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Construct similarity graph\n",
    "similarity_matrix = cosine_similarity(X_scaled)\n",
    "threshold = 0.7  # Adjust threshold as required\n",
    "adjacency_matrix = (similarity_matrix > threshold).astype(int)\n",
    "edge_index = dense_to_sparse(torch.tensor(adjacency_matrix, dtype=torch.float))[0]\n",
    "\n",
    "# Create PyTorch Geometric data object\n",
    "data = Data(x=X_tensor, edge_index=edge_index, y=y_tensor)\n",
    "\n",
    "torch.save(data, 'graphsage_processed_data.pt')\n",
    "print(\"Graph data saved to 'graphsage_processed_data.pt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5488316b-e187-49cf-9888-7fe80294117b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jx/jr14ll9n72g3dqzzmczc16fw0000gn/T/ipykernel_69230/109228600.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load('graphsage_processed_data.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded graph data from 'graphsage_processed_data.pt'.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch\n",
    "\n",
    "# Load preprocessed data\n",
    "data = torch.load('graphsage_processed_data.pt')\n",
    "print(\"Loaded graph data from 'graphsage_processed_data.pt'.\")\n",
    "\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "in_channels = data.x.size(1)\n",
    "hidden_channels = 64\n",
    "out_channels = in_channels  # For reconstruction\n",
    "\n",
    "# Initialize model and optimizer\n",
    "sage_model = GraphSAGE(in_channels, hidden_channels, out_channels)\n",
    "optimizer = torch.optim.Adam(sage_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Training loop\n",
    "def train_sage(model, data, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(data.x, data.edge_index)\n",
    "        loss = F.mse_loss(embeddings, data.x)  # Reconstruction loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "    \n",
    "    # Save trained model\n",
    "    torch.save(model.state_dict(), 'graphsage_model.pt')\n",
    "    print(\"Trained GraphSAGE model saved to 'graphsage_model.pt'.\")\n",
    "\n",
    "train_sage(sage_model, data, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4eaf673-76e0-405b-8d6e-15fd1dff6e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jx/jr14ll9n72g3dqzzmczc16fw0000gn/T/ipykernel_69610/3108841699.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load('graphsage_processed_data.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded graph data from 'graphsage_processed_data.pt'.\n",
      "Epoch 1, Loss: 1.381186842918396\n",
      "Epoch 2, Loss: 1.2261302471160889\n",
      "Epoch 3, Loss: 1.115859031677246\n",
      "Trained GraphSAGE model saved to 'graphsage_model.pt'.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch\n",
    "\n",
    "\n",
    "data = torch.load('graphsage_processed_data.pt')\n",
    "print(\"Loaded graph data from 'graphsage_processed_data.pt'.\")\n",
    "\n",
    "# Define the GraphSAGE model\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "in_channels = data.x.size(1)\n",
    "hidden_channels = 8  # Reduced size\n",
    "out_channels = in_channels  # For reconstruction\n",
    "\n",
    "# Initialize model and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "sage_model = GraphSAGE(in_channels, hidden_channels, out_channels).to(device)\n",
    "optimizer = torch.optim.Adam(sage_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Training loop\n",
    "def train_sage(model, data, epochs=3):  # Reduced epochs\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(data.x, data.edge_index)\n",
    "        loss = F.mse_loss(embeddings, data.x)  # Reconstruction loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "    \n",
    "    # Save trained model\n",
    "    torch.save(model.state_dict(), 'graphsage_model.pt')\n",
    "    print(\"Trained GraphSAGE model saved to 'graphsage_model.pt'.\")\n",
    "\n",
    "train_sage(sage_model, data, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d018c0e0-b35a-4dc4-9451-90eb1dde356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model: torch.nn.Module, data: Data) -> np.ndarray:\n",
    "    \"\"\"Extract embeddings from the GraphSAGE model.\"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data.x, data.edge_index)  # This might return multiple values\n",
    "        encoded_features = outputs  # Since GraphSAGE only returns embeddings (usually)\n",
    "    return encoded_features.numpy()  # Return the embeddings as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56fba8ed-6362-4111-9400-3eef2f5a63fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9669, number of negative: 22331\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3570\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.302156 -> initscore=-0.837051\n",
      "[LightGBM] [Info] Start training from score -0.837051\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.90      0.82      5549\n",
      "           1       0.58      0.31      0.40      2451\n",
      "\n",
      "    accuracy                           0.72      8000\n",
      "   macro avg       0.67      0.61      0.61      8000\n",
      "weighted avg       0.70      0.72      0.69      8000\n",
      "\n",
      "Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Split the selected features into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train LightGBM classifier\n",
    "lgb_classifier = LGBMClassifier(random_state=42)\n",
    "lgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = lgb_classifier.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e78e44d1-a134-4371-a197-6830ec8128d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 9669, number of negative: 22331\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3570\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.302156 -> initscore=-0.837051\n",
      "[LightGBM] [Info] Start training from score -0.837051\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.90      0.82      5549\n",
      "           1       0.58      0.31      0.40      2451\n",
      "\n",
      "    accuracy                           0.72      8000\n",
      "   macro avg       0.67      0.61      0.61      8000\n",
      "weighted avg       0.70      0.72      0.69      8000\n",
      "\n",
      "Accuracy: 0.72\n",
      "GraphSAGE predictions saved.\n",
      "GraphSAGE LightGBM model saved.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Assuming 'X_selected' and 'y' are already generated from GraphSAGE embeddings\n",
    "# Split the selected features into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train LightGBM classifier\n",
    "lgb_classifier = LGBMClassifier(random_state=42)\n",
    "lgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = lgb_classifier.predict(X_test)\n",
    "y_prob = lgb_classifier.predict_proba(X_test)  # Optional: Prediction probabilities\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Save predictions and model\n",
    "np.save(\"graphsage_predictions.npy\", y_pred)  # Save predictions\n",
    "np.save(\"graphsage_probabilities.npy\", y_prob)  # Save probabilities\n",
    "print(\"GraphSAGE predictions saved.\")\n",
    "\n",
    "joblib.dump(lgb_classifier, \"graphsage_lightgbm.pkl\")  # Save model\n",
    "print(\"GraphSAGE LightGBM model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "225b6f84-d330-4e01-a04c-7291932ac11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to 'processed_graph_data.pt'.\n",
      "Model state saved to 'graph_autoencoder_model.pt'.\n"
     ]
    }
   ],
   "source": [
    "# GAE DIFFERENT MODEL\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import numpy as np\n",
    "labeled_data = pd.read_csv('3k_labeled_extracted.csv')\n",
    "unlabeled_data = pd.read_csv('6k_unlabeled_extracted.csv')\n",
    "\n",
    "labeled_data = labeled_data.drop(columns=['text_'])\n",
    "unlabeled_data = unlabeled_data.drop(columns=['text_'])\n",
    "\n",
    "X_labeled = labeled_data.drop(columns=['label']).values\n",
    "y_labeled = labeled_data['label'].values\n",
    "X_unlabeled = unlabeled_data.values\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_labeled_scaled = scaler.fit_transform(X_labeled)\n",
    "X_unlabeled_scaled = scaler.transform(X_unlabeled)\n",
    "\n",
    "X_labeled_tensor = torch.tensor(X_labeled_scaled, dtype=torch.float)\n",
    "y_labeled_tensor = torch.tensor(y_labeled, dtype=torch.long)\n",
    "X_unlabeled_tensor = torch.tensor(X_unlabeled_scaled, dtype=torch.float)\n",
    "\n",
    "similarity_matrix = cosine_similarity(np.vstack([X_labeled_scaled, X_unlabeled_scaled]))\n",
    "\n",
    "threshold = 0.7  # Adjust threshold for graph sparsity\n",
    "adjacency_matrix = (similarity_matrix > threshold).astype(int)\n",
    "\n",
    "\n",
    "edge_index = dense_to_sparse(torch.tensor(adjacency_matrix, dtype=torch.float))[0]\n",
    "\n",
    "\n",
    "features = torch.cat([X_labeled_tensor, X_unlabeled_tensor], dim=0)\n",
    "data = Data(x=features, edge_index=edge_index)\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'features': features,\n",
    "    'edge_index': edge_index,\n",
    "    'labels': y_labeled_tensor\n",
    "}, 'processed_graph_data.pt')\n",
    "print(\"Processed data saved to 'processed_graph_data.pt'.\")\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GraphAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphAutoencoder, self).__init__()\n",
    "        self.encoder = GCNConv(in_channels, hidden_channels)\n",
    "        self.decoder = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Encoder: learn low-dimensional embeddings\n",
    "        encoded = F.relu(self.encoder(x, edge_index))\n",
    "        # Decoder: reconstruct from embeddings\n",
    "        decoded = self.decoder(encoded, edge_index)\n",
    "        return decoded, encoded\n",
    "\n",
    "# Model parameters\n",
    "in_channels = features.size(1)\n",
    "hidden_channels = 64\n",
    "out_channels = in_channels\n",
    "\n",
    "# Initialize the GAE model\n",
    "gae_model = GraphAutoencoder(in_channels, hidden_channels, out_channels)\n",
    "optimizer = torch.optim.Adam(gae_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Saving the model state\n",
    "torch.save(gae_model.state_dict(), 'graph_autoencoder_model.pt')\n",
    "print(\"Model state saved to 'graph_autoencoder_model.pt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d667370-47b9-482b-994a-a2ee2d4e9e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3672053813934326, Validation Loss: 2.3672053813934326\n",
      "Epoch 1, Loss: 1.8817138671875, Validation Loss: 1.8817138671875\n",
      "Epoch 2, Loss: 1.5160062313079834, Validation Loss: 1.5160062313079834\n",
      "Epoch 3, Loss: 1.2371913194656372, Validation Loss: 1.2371913194656372\n",
      "Epoch 4, Loss: 1.033754825592041, Validation Loss: 1.033754825592041\n",
      "Trained model state saved to 'graph_autoencoder_model_trained.pt'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GraphAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphAutoencoder, self).__init__()\n",
    "        self.encoder = GCNConv(in_channels, hidden_channels)\n",
    "        self.decoder = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        encoded = F.relu(self.encoder(x, edge_index))\n",
    "        decoded = self.decoder(encoded, edge_index)\n",
    "        return decoded, encoded\n",
    "\n",
    "# Initialize the model parameters\n",
    "in_channels = features.size(1)\n",
    "hidden_channels = 64\n",
    "out_channels = in_channels\n",
    "\n",
    "# Initialize the GAE model and optimizer\n",
    "gae_model = GraphAutoencoder(in_channels, hidden_channels, out_channels)\n",
    "optimizer = torch.optim.Adam(gae_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train_gae(data, epochs=5):\n",
    "    \"\"\"Train the Graph Autoencoder and return training and validation losses.\"\"\"\n",
    "    gae_model.train()\n",
    "    \n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []  # For future use if you implement validation\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        decoded, encoded = gae_model(data.x, data.edge_index)\n",
    "\n",
    "\n",
    "        loss = F.mse_loss(decoded, data.x)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())  # Append training loss\n",
    "\n",
    "\n",
    "        val_loss = F.mse_loss(decoded, data.x)  # Use the same data for simplicity, replace with validation data in practice\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "            \n",
    "\n",
    "\n",
    "    torch.save(gae_model.state_dict(), 'graph_autoencoder_model_trained.pt')\n",
    "    print(\"Trained model state saved to 'graph_autoencoder_model_trained.pt'.\")\n",
    "\n",
    "    return train_losses, val_losses  # Return the lists of losses\n",
    "\n",
    "\n",
    "train_losses, val_losses = train_gae(data, epochs=5)  # Adjust epochs as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b056f39-5aff-40c6-9d62-fa0dc0e05a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1484, number of negative: 1516\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16320\n",
      "[LightGBM] [Info] Number of data points in the train set: 3000, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.494667 -> initscore=-0.021334\n",
      "[LightGBM] [Info] Start training from score -0.021334\n",
      "\n",
      "Evaluating LightGBM Classifier...\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      1516\n",
      "           1       0.98      0.99      0.99      1484\n",
      "\n",
      "    accuracy                           0.99      3000\n",
      "   macro avg       0.99      0.99      0.99      3000\n",
      "weighted avg       0.99      0.99      0.99      3000\n",
      "\n",
      "Accuracy: 98.57%\n",
      "Unlabeled data with predicted labels saved to 'unlabeled_data_predictions_22_nov.csv'.\n",
      "PyTorch model saved as 'gae_model_22_nov.pth'.\n",
      "LightGBM classifier saved as 'lgb_classifier_22_nov.pkl'.\n",
      "\n",
      "Head of the new predictions:\n",
      "   rating  Positive Score  Negative Score  Neutral Score  Compound Score  \\\n",
      "0     3.0           0.000           0.000          1.000          0.0000   \n",
      "1     1.0           0.073           0.253          0.674         -0.8685   \n",
      "2     5.0           0.598           0.000          0.402          0.9246   \n",
      "3     1.0           0.158           0.094          0.747          0.7500   \n",
      "4     5.0           0.203           0.054          0.743          0.9287   \n",
      "\n",
      "   Review Length  Noun Count  Verb Count  Adjective Count  Joy Score  \\\n",
      "0              1           1           0                0      0.000   \n",
      "1             42          12           9                5      0.073   \n",
      "2             14           5           2                2      0.598   \n",
      "3            140          45          27               11      0.158   \n",
      "4             79          22          19                5      0.203   \n",
      "\n",
      "   Sadness Score  BERT Component 1  BERT Component 2  BERT Component 3  \\\n",
      "0          0.000         -5.235278         -0.133021         -0.114847   \n",
      "1          0.253          4.228276         -0.769990         -0.247665   \n",
      "2          0.000          3.033373         -2.084538          0.154726   \n",
      "3          0.094          3.802375         -0.908699          0.063371   \n",
      "4          0.054          4.029655         -0.985872         -0.374943   \n",
      "\n",
      "   predicted_label  \n",
      "0                1  \n",
      "1                1  \n",
      "2                1  \n",
      "3                0  \n",
      "4                1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "\n",
    "def extract_embeddings(model: torch.nn.Module, data: Data) -> np.ndarray:\n",
    "    \"\"\"Extract embeddings from the graph autoencoder model.\"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        _, encoded_features = model(data.x, data.edge_index)\n",
    "    return encoded_features.numpy()\n",
    "\n",
    "\n",
    "def evaluate_classifier(classifier, X_train: np.ndarray, y_train: np.ndarray) -> None:\n",
    "    \"\"\"Evaluate the trained model and print classification report.\"\"\"\n",
    "    y_pred = classifier.predict(X_train)  # Predict on the labeled training data\n",
    "    report = classification_report(y_train, y_pred, output_dict=True)\n",
    "    accuracy = report['accuracy'] * 100  # Convert to percentage\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(classification_report(y_train, y_pred))\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return accuracy, y_pred\n",
    "\n",
    "\n",
    "def predict_unlabeled_data(classifier, encoded_features: np.ndarray, num_labeled: int) -> np.ndarray:\n",
    "    \"\"\"Predict labels for unlabeled data using the trained classifier.\"\"\"\n",
    "    X_unlabeled_features = encoded_features[num_labeled:]  # Get unlabeled features\n",
    "    return classifier.predict(X_unlabeled_features)\n",
    "\n",
    "\n",
    "def save_predictions(unlabeled_data: pd.DataFrame, predictions: np.ndarray, filename: str) -> None:\n",
    "    \"\"\"Add predicted labels to the unlabeled data and save to a CSV file.\"\"\"\n",
    "    unlabeled_data['predicted_label'] = predictions\n",
    "    unlabeled_data.to_csv(filename, index=False)\n",
    "    print(f\"Unlabeled data with predicted labels saved to '{filename}'.\")\n",
    "\n",
    "\n",
    "def train_and_evaluate(classifier, X_train: np.ndarray, y_train: np.ndarray, model_name: str) -> None:\n",
    "    \"\"\"Train the classifier and evaluate it.\"\"\"\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    accuracy, y_pred = evaluate_classifier(classifier, X_train, y_train)\n",
    "    \n",
    "    return accuracy, y_pred\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Step 1: Extract embeddings\n",
    "    encoded_features = extract_embeddings(gae_model, data)\n",
    "    \n",
    "    num_labeled = X_labeled_tensor.size(0)  # Get number of labeled samples\n",
    "    X_train = encoded_features[:num_labeled]  # Labeled features\n",
    "    y_train = y_labeled_tensor.numpy()  # Labeled targets\n",
    "\n",
    "    # Step 2: Train LightGBM Classifier\n",
    "    lgb_classifier = lgb.LGBMClassifier(random_state=42)\n",
    "    accuracy, y_pred = train_and_evaluate(lgb_classifier, X_train, y_train, \"LightGBM Classifier\")\n",
    "\n",
    "    # Step 3: Predict labels for unlabeled data\n",
    "    predicted_labels = predict_unlabeled_data(lgb_classifier, encoded_features, num_labeled)\n",
    "\n",
    "    # Step 4: Save predictions\n",
    "    save_predictions(unlabeled_data, predicted_labels, \"unlabeled_data_predictions_22_nov.csv\")\n",
    "\n",
    "    # Step 5: Save models\n",
    "    torch.save(gae_model.state_dict(), \"gae_model_22_nov.pth\")\n",
    "    print(\"PyTorch model saved as 'gae_model_22_nov.pth'.\")\n",
    "\n",
    "    joblib.dump(lgb_classifier, \"lgb_classifier_22_nov.pkl\")\n",
    "    print(\"LightGBM classifier saved as 'lgb_classifier_22_nov.pkl'.\")\n",
    "\n",
    "    # Display head of predictions\n",
    "    print(\"\\nHead of the new predictions:\")\n",
    "    print(unlabeled_data.head())  # Display the first few rows\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "# END OF GAE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "009fd731-ef95-4fbf-b08b-c6468b065b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAE + LightGBM model saved as 'gae_lightgbm_model.pkl'.\n",
      "Unlabeled data with predicted labels saved to 'unlabeled_data_predictions_gae.csv'.\n",
      "GAE model saved as 'gae_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "joblib.dump(lgb_classifier, \"gae_lightgbm_model.pkl\")\n",
    "print(\"GAE + LightGBM model saved as 'gae_lightgbm_model.pkl'.\")\n",
    "predicted_labels = predict_unlabeled_data(lgb_classifier, encoded_features, num_labeled)\n",
    "unlabeled_data = pd.DataFrame(encoded_features[num_labeled:], columns=[f\"feature_{i}\" for i in range(encoded_features.shape[1])])  # Create dataframe for unlabeled data\n",
    "save_predictions(unlabeled_data, predicted_labels, \"unlabeled_data_predictions_gae.csv\")\n",
    "\n",
    "torch.save(gae_model.state_dict(), \"gae_model.pth\")\n",
    "print(\"GAE model saved as 'gae_model.pth'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "80af4eb4-02d1-4edb-b747-5b97d165c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model: torch.nn.Module, data: Data) -> np.ndarray:\n",
    "    \"\"\"Extract embeddings from the GraphSAGE or Graph Autoencoder model.\"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data.x, data.edge_index)  # This might return multiple values\n",
    "        \n",
    "        # If the model is a Graph Autoencoder (GAE), unpack the tuple\n",
    "        if isinstance(outputs, tuple):\n",
    "            _, encoded_features = outputs  # GAE returns a tuple (decoded, encoded)\n",
    "        else:\n",
    "            encoded_features = outputs  # For GraphSAGE, we just get the embeddings directly\n",
    "    \n",
    "    return encoded_features.numpy()  # Return the embeddings as a numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b74e116a-ad2a-4aac-a7ce-dbe38145fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphsage_embeddings = extract_embeddings(sage_model, data)  # From GraphSAGE\n",
    "gae_embeddings = extract_embeddings(gae_model, data)  # From GAE\n",
    "\n",
    "num_labeled = X_labeled_tensor.size(0)\n",
    "combined_embeddings = np.concatenate([graphsage_embeddings, gae_embeddings], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0c00b337-d2c9-465d-99df-65e9132e77c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1124, number of negative: 1126\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19890\n",
      "[LightGBM] [Info] Number of data points in the train set: 2250, number of used features: 78\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499556 -> initscore=-0.001778\n",
      "[LightGBM] [Info] Start training from score -0.001778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.73      0.75       390\n",
      "           1       0.73      0.77      0.75       360\n",
      "\n",
      "    accuracy                           0.75       750\n",
      "   macro avg       0.75      0.75      0.75       750\n",
      "weighted avg       0.75      0.75      0.75       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_combined = combined_embeddings[:num_labeled] \n",
    "y_combined = y_labeled_tensor.numpy() \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, random_state=42)\n",
    "\n",
    "lgb_classifier = lgb.LGBMClassifier(random_state=42)\n",
    "lgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lgb_classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
