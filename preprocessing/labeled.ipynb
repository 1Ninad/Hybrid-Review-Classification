import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Ensure NLTK resources are downloaded
nltk.download('stopwords')
nltk.download('wordnet')

# Load the datasets
yelp_df = pd.read_csv('/content/Labelled Yelp Dataset.csv')  # Adjust path if necessary
fake_reviews_df = pd.read_csv('/content/fake reviews dataset.csv')  # Adjust path if necessary

# Step 1: Rename columns for consistency and filter required columns
yelp_df.rename(columns={'Review': 'review', 'Rating': 'rating', 'Label': 'label'}, inplace=True)
fake_reviews_df.rename(columns={'text_': 'review', 'rating': 'rating', 'label': 'label'}, inplace=True)

# Step 2: Filter to keep only the columns review, rating, and label
yelp_df = yelp_df[['review', 'rating', 'label']]
fake_reviews_df = fake_reviews_df[['review', 'rating', 'label']]

# Step 3: Convert fake reviews to consistent labels (CG, OR, -1, 1)
fake_reviews_df['label'] = fake_reviews_df['label'].astype(str)
fake_reviews_df['label'] = fake_reviews_df['label'].replace({'CG': '-1', '-1': '-1', 'OR': '1', '1': '1'})
fake_reviews_df['label'] = fake_reviews_df['label'].astype(int)

# Step 4: Sample 30,000 reviews from Yelp dataset
yelp_sampled_df = yelp_df.sample(n=30000, random_state=42)

# Step 5: Combine the Yelp and Fake Reviews datasets
combined_df = pd.concat([yelp_sampled_df, fake_reviews_df], ignore_index=True)

# Text preprocessing: Stopword removal and lemmatization
def preprocess_review(text):
    # Normalize the text
    text = text.lower()
    
    # Remove special characters and numbers
    text = re.sub(r'[^a-z\s]', '', text)
    
    # Initialize lemmatizer and stop words
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))
    
    # Remove stopwords and lemmatize
    lemmatized_text = ' '.join(
        lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words
    )
    
    return lemmatized_text

# Apply the preprocessing to the combined dataset
combined_df['processed_review'] = combined_df['review'].apply(preprocess_review)

# Save the combined and processed dataset to a CSV file
combined_csv_path = '/content/combined_reviews_processed.csv'
combined_df.to_csv(combined_csv_path, index=False)

# Display the first five and last five rows of the combined data
print("First five reviews:")
print(combined_df[['review', 'rating', 'label', 'processed_review']].head())

print("\nLast five reviews:")
print(combined_df[['review', 'rating', 'label', 'processed_review']].tail())











import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/My Drive/combined_reviews.csv')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    # Convert text to lowercase
    text = text.lower()

    text = re.sub(r'[^a-z\s]', '', text)

    tokens = text.split()
    tokens = [word for word in tokens if word not in stop_words]

    
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    return ' '.join(tokens)

data['review'] = data['review'].apply(preprocess_text)

data.to_csv('/content/drive/My Drive/preprocessed.csv', index=False)

data.head()

import pandas as pd
import torch
import numpy as np
import nltk
from nltk import pos_tag
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from transformers import BertTokenizer, BertModel
from sklearn.decomposition import PCA
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

from google.colab import drive
drive.mount('/content/drive')
data = pd.read_csv('/content/drive/My Drive/preprocessed.csv')

vader_analyzer = SentimentIntensityAnalyzer()

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

stop_words = set(stopwords.words('english'))

data['review'] = data['review'].fillna('')


positive_scores = []
negative_scores = []
neutral_scores = []
compound_scores = []
review_lengths = []
noun_counts = []
verb_counts = []
adjective_counts = []
joy_scores = []
sadness_scores = []
bert_embeddings = []


for review in data['review']:
    # VADER sentiment scores
    pos, neg, neu, comp = vader_sentiment(review)
    positive_scores.append(pos)
    negative_scores.append(neg)
    neutral_scores.append(neu)
    compound_scores.append(comp)

    review_lengths.append(len(review.split()))
    noun_count, verb_count, adj_count = count_pos_tags(review)
    noun_counts.append(noun_count)
    verb_counts.append(verb_count)
    adjective_counts.append(adj_count)

   
    joy_scores.append(pos)
    sadness_scores.append(neg)

   
    bert_embedding = get_bert_embeddings(review)
    bert_embeddings.append(bert_embedding)

bert_embeddings_np = np.array(bert_embeddings)


pca = PCA(n_components=3)
bert_pca = pca.fit_transform(bert_embeddings_np)

data['Positive Score'] = positive_scores
data['Negative Score'] = negative_scores
data['Neutral Score'] = neutral_scores
data['Compound Score'] = compound_scores
data['Review Length'] = review_lengths
data['Noun Count'] = noun_counts
data['Verb Count'] = verb_counts
data['Adjective Count'] = adjective_counts
data['Joy Score'] = joy_scores
data['Sadness Score'] = sadness_scores
data['BERT Component 1'] = bert_pca[:, 0]
data['BERT Component 2'] = bert_pca[:, 1]
data['BERT Component 3'] = bert_pca[:, 2]

output_path = '/content/drive/My Drive/bert_vader.csv'
data.to_csv(output_path, index=False)
data.head()




