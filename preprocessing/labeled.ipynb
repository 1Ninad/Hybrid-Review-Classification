import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Ensure NLTK resources are downloaded
nltk.download('stopwords')
nltk.download('wordnet')

# Load the datasets
yelp_df = pd.read_csv('/content/Labelled Yelp Dataset.csv')  # Adjust path if necessary
fake_reviews_df = pd.read_csv('/content/fake reviews dataset.csv')  # Adjust path if necessary

# Step 1: Rename columns for consistency and filter required columns
yelp_df.rename(columns={'Review': 'review', 'Rating': 'rating', 'Label': 'label'}, inplace=True)
fake_reviews_df.rename(columns={'text_': 'review', 'rating': 'rating', 'label': 'label'}, inplace=True)

# Step 2: Filter to keep only the columns review, rating, and label
yelp_df = yelp_df[['review', 'rating', 'label']]
fake_reviews_df = fake_reviews_df[['review', 'rating', 'label']]

# Step 3: Convert fake reviews to consistent labels (CG, OR, -1, 1)
fake_reviews_df['label'] = fake_reviews_df['label'].astype(str)
fake_reviews_df['label'] = fake_reviews_df['label'].replace({'CG': '-1', '-1': '-1', 'OR': '1', '1': '1'})
fake_reviews_df['label'] = fake_reviews_df['label'].astype(int)

# Step 4: Sample 30,000 reviews from Yelp dataset
yelp_sampled_df = yelp_df.sample(n=30000, random_state=42)

# Step 5: Combine the Yelp and Fake Reviews datasets
combined_df = pd.concat([yelp_sampled_df, fake_reviews_df], ignore_index=True)

# Text preprocessing: Stopword removal and lemmatization
def preprocess_review(text):
    # Normalize the text
    text = text.lower()
    
    # Remove special characters and numbers
    text = re.sub(r'[^a-z\s]', '', text)
    
    # Initialize lemmatizer and stop words
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))
    
    # Remove stopwords and lemmatize
    lemmatized_text = ' '.join(
        lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words
    )
    
    return lemmatized_text

# Apply the preprocessing to the combined dataset
combined_df['processed_review'] = combined_df['review'].apply(preprocess_review)

# Save the combined and processed dataset to a CSV file
combined_csv_path = '/content/combined_reviews_processed.csv'
combined_df.to_csv(combined_csv_path, index=False)

# Display the first five and last five rows of the combined data
print("First five reviews:")
print(combined_df[['review', 'rating', 'label', 'processed_review']].head())

print("\nLast five reviews:")
print(combined_df[['review', 'rating', 'label', 'processed_review']].tail())

