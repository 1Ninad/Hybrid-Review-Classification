import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk import pos_tag
from transformers import BertTokenizer, BertModel
from sklearn.decomposition import PCA
import numpy as np
import torch

# Ensure NLTK resources are downloaded
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('vader_lexicon')

# Load the datasets
iphone_data = pd.read_csv('apple_iphone_11_reviews.csv')
restaurant_data = pd.read_csv('reviews.csv')

# Step 1: Extract required columns from each dataset
iphone_data = iphone_data[['review_text', 'review_rating']]
restaurant_data = restaurant_data[['text', 'rating']]

# Step 2: Clean and convert iPhone ratings
iphone_data['review_rating'] = iphone_data['review_rating'].apply(
    lambda x: float(re.search(r'(\d+(\.\d+)?)', x).group(0)) if re.search(r'(\d+(\.\d+)?)', x) else None
)

# Ensure the restaurant ratings are float
restaurant_data['rating'] = restaurant_data['rating'].astype(float)

# Step 3: Rename columns for consistency
iphone_data.rename(columns={'review_text': 'review', 'review_rating': 'rating'}, inplace=True)
restaurant_data.rename(columns={'text': 'review', 'rating': 'rating'}, inplace=True)

# Step 4: Combine the datasets
combined_data = pd.concat([iphone_data, restaurant_data], ignore_index=True)

# Initialize NLTK components
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()
sia = SentimentIntensityAnalyzer()

# Function for text preprocessing
def preprocess_review(text):
    if not isinstance(text, str):
        return ''

    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)

    words = nltk.word_tokenize(text)
    words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words if word.isalpha() and word not in stop_words]

    return ' '.join(words)

# Apply preprocessing
combined_data['processed_review'] = combined_data['review'].apply(preprocess_review)

# Function to calculate sentiment and features
def analyze_review(review):
    if not isinstance(review, str):
        return {
            "Positive Score": 0,
            "Negative Score": 0,
            "Neutral Score": 0,
            "Compound Score": 0,
            "Review Length": 0,
            "Noun Count": 0,
            "Verb Count": 0,
            "Adjective Count": 0,
            "Joy Score": 0,
            "Sadness Score": 0,
        }

    pos_tags = pos_tag(nltk.word_tokenize(review))

    # Feature counts
    noun_count = sum(1 for _, tag in pos_tags if tag.startswith('NN'))
    verb_count = sum(1 for _, tag in pos_tags if tag.startswith('VB'))
    adjective_count = sum(1 for _, tag in pos_tags if tag.startswith('JJ'))

    # Sentiment scores using VADER
    sentiment_scores = sia.polarity_scores(review)

    # Calculate Joy and Sadness Scores
    joy_score = sentiment_scores['pos']
    sadness_score = sentiment_scores['neg']

    return {
        "Positive Score": sentiment_scores['pos'],
        "Negative Score": sentiment_scores['neg'],
        "Neutral Score": sentiment_scores['neu'],
        "Compound Score": sentiment_scores['compound'],
        "Review Length": len(review.split()),
        "Noun Count": noun_count,
        "Verb Count": verb_count,
        "Adjective Count": adjective_count,
        "Joy Score": joy_score,
        "Sadness Score": sadness_score,
    }

# Analyze each review and extract features
feature_results = [analyze_review(review) for review in combined_data['review']]
features_df = pd.DataFrame(feature_results)

# Function to obtain BERT embeddings and reduce dimensions using PCA
def get_bert_embeddings(reviews):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained('bert-base-uncased')

    embeddings = []

    for review in reviews:
        if not isinstance(review, str):
            continue  # Skip non-string reviews

        inputs = tokenizer(review, return_tensors='pt', padding=True, truncation=True)
        outputs = model(**inputs)
        review_embedding = outputs.last_hidden_state.mean(dim=1).detach().numpy()
        embeddings.append(review_embedding[0])

    embeddings_np = np.array(embeddings)

    # Reduce dimensionality using PCA
    pca = PCA(n_components=3)
    reduced_embeddings = pca.fit_transform(embeddings_np)

    return reduced_embeddings

# Get BERT embeddings and reduce dimensions
bert_embeddings = get_bert_embeddings(combined_data['processed_review'])
features_df[['BERT Component 1', 'BERT Component 2', 'BERT Component 3']] = bert_embeddings

# Combine features with original dataset
combined_data = pd.concat([combined_data, features_df], axis=1)

# Save combined dataset with features to a CSV file
output_file_path = r'C:\Users\Amey\edicsv\combined_reviews_with_features.csv'  # Update path
combined_data.to_csv(output_file_path, index=False)

# Display the first five and last five rows of the combined data
print("First five reviews:")
print(combined_data.head())

print("\nLast five reviews:")
print(combined_data.tail())
print(f"\nData saved to {output_file_path}")


