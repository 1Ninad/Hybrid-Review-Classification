import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.sentiment import SentimentIntensityAnalyzer
from transformers import pipeline

# Ensure NLTK resources are downloaded
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('vader_lexicon')  # Ensure VADER lexicon is downloaded

# Load the datasets
iphone_data = pd.read_csv('apple_iphone_11_reviews.csv')  # Assuming the iPhone data is in a CSV file
restaurant_data = pd.read_csv('reviews.csv')  # Assuming the restaurant data is in a CSV file

# Step 1: Extract required columns from each dataset
iphone_data = iphone_data[['review_text', 'review_rating']]
restaurant_data = restaurant_data[['text', 'rating']]

# Step 2: Clean and convert iPhone ratings
iphone_data['review_rating'] = iphone_data['review_rating'].apply(
    lambda x: float(re.search(r'(\d+(\.\d+)?)', x).group(0)) if re.search(r'(\d+(\.\d+)?)', x) else None
)

# Ensure the restaurant ratings are float
restaurant_data['rating'] = restaurant_data['rating'].astype(float)

# Step 3: Rename columns for consistency
iphone_data.rename(columns={'review_text': 'review', 'review_rating': 'rating'}, inplace=True)
restaurant_data.rename(columns={'text': 'review', 'rating': 'rating'}, inplace=True)

# Step 4: Combine the datasets
combined_data = pd.concat([iphone_data, restaurant_data], ignore_index=True)

# Text preprocessing: Stopword removal and lemmatization
def preprocess_review(text):
    # Check if the input is a string
    if not isinstance(text, str):
        return ''  # Return an empty string for non-string inputs

    # Normalize the text
    text = text.lower()
    # Remove special characters and numbers
    text = re.sub(r'[^a-z\s]', '', text)

    # Initialize lemmatizer and stop words
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))

    # Remove stopwords and lemmatize
    lemmatized_text = ' '.join(
        lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words
    )

    return lemmatized_text

# Apply the preprocessing to the combined dataset
combined_data['processed_review'] = combined_data['review'].apply(preprocess_review)

# Initialize VADER sentiment analyzer
sia = SentimentIntensityAnalyzer()

# Initialize BERT sentiment analysis pipeline
bert_sentiment = pipeline("sentiment-analysis")

# Function to apply VADER
def analyze_vader(text):
    scores = sia.polarity_scores(text)
    return scores['compound']  # Return the compound score

# Function to apply BERT
def analyze_bert(text):
    result = bert_sentiment(text)
    return result[0]['score'] if result else None  # Return the score from the BERT output

# Apply VADER and BERT to the processed reviews
combined_data['vader_sentiment'] = combined_data['processed_review'].apply(analyze_vader)
combined_data['bert_sentiment'] = combined_data['processed_review'].apply(analyze_bert)

# Optional: Save combined dataset with processed reviews and sentiment scores to a CSV file
combined_data.to_csv('combined_reviews_with_sentiment.csv', index=False)

# Display the first five and last five rows of the combined data
print("First five reviews:")
print(combined_data[['processed_review', 'vader_sentiment', 'bert_sentiment']].head())

print("\nLast five reviews:")
print(combined_data[['processed_review', 'vader_sentiment', 'bert_sentiment']].tail())

